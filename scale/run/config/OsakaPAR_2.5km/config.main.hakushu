#!/bin/bash
#===============================================================================
#
#  Main settings for SCALE-LETKF scripts
#
#===============================================================================

DIR="$(cd "$(pwd)/.." && pwd)"   # Root directory of the SCALE-LETKF

OUTDIR="/data1/gylien/exp/test"  # Directory for SCALE-LETKF output

LOGDIR="$DIR/log"

#===============================================================================
# Location of model/data files

MODELDIR="/data1/gylien/scale/scale-les/test/case_real/test_ncepfnl5/run_WRF"
DATADIR="/data1/SCALE/database"

DATA_BDY_WRF="/data1/SCALE/wrfout_10min"
BDYINT=600
#ANLGFS="/discover/nobackup/glien/model/CFSR_t62"         # directory of reference model files [sigma/surface formats]
#ANLGRD="/discover/nobackup/glien/model/CFSR_t62_grd"     # directory of reference model files [sigma-level grid format]
#ANLGRDP="/discover/nobackup/glien/model/CFSR_t62_grdp"   # directory of reference model files [pressure-level grid format]
#ANLGRDP2="/discover/nobackup/glien/model/EC_interim_t62_grdp" # directory of reference model files [pressure-level grid format]

#INITGFS="/discover/nobackup/glien/model/GDAS_IC_t62"     # directory of arbitrary initial condition files [sigma/surface formats]

OBS="/data1/gylien/obs/test_220x180_radar"     # directory of observation data in LETKF obs format
OBSNCEP="/data1/gylien/obs/prepbufr"  # directory of observation data in NCEP BUFR format

#===============================================================================
# Cycling settings

WINDOW_S=300       # SCALE forecast time when assimilation window starts (second)
WINDOW_E=900       # SCALE forecast time when assimilation window ends (second)
LCYCLE=600         # Length of a GFS-LETKF cycle (second)
LTIMESLOT=60       # Timeslot interval for 4D-LETKF (second)

#===============================================================================
# Parallelization settings

MEMBER=4           # Ensemble size

NNODES=1           # Number of nodes
PPN=40             # Number of processes per node

THREADS=1          # Number of threads per process

SCALE_NP=4         # Number of processes to run SCALE

BGJOB_INT='0.1s'   # Interval of multiple background job submissions

#===============================================================================
# Temporary directories to store runtime files

MACHINE_TYPE=1              # Machine type
                            #  1: Linux cluster with PBS
                            # 10: K-computer
                            # 11: K-computer (micro jobs)
                            # 12: K-computer (interact jobs)

TMPDAT_MODE=3               # Disk type used for the 'dat' temporary directory (input data)
TMPRUN_MODE=3               # Disk type used for the 'run' temporary directory (runtime files)
TMPOUT_MODE=$TMPRUN_MODE    # Disk type used for the 'out' temporary directory (output)
                            #  1: share (link to TMP)
                            #  2: share (staging to TMP)
                            #  3: local (staging to TMPL)

ONLINE_STGOUT=1             # Stage out right after each cycle (do not wait until the end of the job)?
                            #  0: No
                            #  1: Yes

SYSNAME="$(basename $OUTDIR)"                # A unique name in the machine
TMPSUBDIR="scale-letkf_$(whoami)_${SYSNAME}" # (used to identify multiple runs in the same time)

TMP="$DIR/tmp/$TMPSUBDIR"   # Temporary directory shared among all nodes
TMPS="/dev/shm/$TMPSUBDIR"  # Temporary directory only on the server node
TMPL="/dev/shm/$TMPSUBDIR"  # Local temporary directory on computing nodes

#===============================================================================
# Environmental settings

MPIBIN=$(dirname $(which mpirun))
MPIRUN="$MPIBIN/mpirun"

SCP='cp -L'
SCP_HOSTPREFIX=''
#SCP="scp -q"
#SCP_HOSTPREFIX="XXXX:"

PYTHON="python"

#BUFRBIN=

#===============================================================================
# Machine-independent source file

. config.rc

#===============================================================================
