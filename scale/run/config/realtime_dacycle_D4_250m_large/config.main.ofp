#!/bin/bash
#===============================================================================
#
#  Main settings for SCALE-LETKF scripts
#
#===============================================================================

PRESET='OFP' 

#RSCGRP="challenge"

#===============================================================================

DIR="/work/hp150019/f22013/SCALE-LETKF/scale-5.3.2/scale-5.3.x_LETKF_sngl/scale-letkf-HPC0829/scale"
DIR="$(cd "$(pwd)/.." && pwd)"              # Root directory of the SCALE-LETKF

OUTPUT="/work/hp150019/c24140/scale-letkf-rt/r0051_nest"
<<<<<<< HEAD:scale/run/config/realtime_dacycle_D4/config.main.ofp
OUTPUT="/work/hp150019/f22013/SCALE-LETKF/scale-5.3.2/OUTPUT"
EXP4=D4_250M_NP4096
#EXP4=D4_250M_NP4096_600S_SMTH
#EXP4=D4_250M_NP4096_600S_SMTH_HPC0829_2112_JIT
#EXP4=D4_250M_NP4096_600S_SMTH_HPC0829_1280_NOJIT
#EXP4=D4_250M_NP4096_600S_SMTH_HPC0829_1280_JIT
#
#EXP4=D4_250M_NP4096_600S_SMTH_HPC0829_5248
#EXP4=D4_250M_NP4096_600S_SMTH_HPC0829_3328_NOFCST
#EXP4=D4_250M_NP4096_600S_SMTH_HPC0829_3328_NOFCST_JITON
#EXP4=D4_250M_NP4096_600S_SMTH_HPC0829_3328_NOFCST_JITON2
#
#EXP4=D4_250M_NP4096_600S_SMTH_HPC0829_3968
#EXP4=D4_250M_NP4096_600S_SMTH_HPC0829_6528
#EXP4=D4_250M_NP4096_600S_SMTH_HPC0829_6528
#EXP4=D4_250M_NP4096_600S_SMTH_HPC0829_8192

EXP4=D4_250M_NP4096_600S_SMTH_0913_02

#EXP4=exp_d4
=======
EXP4=exp_d4_250m_large
>>>>>>> dacycle_OFP_dev:scale/run/config/realtime_dacycle_D4_250m_large/config.main.ofp
EXP3=exp_d3
EXP2=exp_d2

OUTDIR="${OUTPUT}/${EXP4}"

<<<<<<< HEAD:scale/run/config/realtime_dacycle_D4/config.main.ofp
INDIR="/work/hp150019/share/honda/SCALE-LETKF/TEST_INPUT/D4_250M_NP4096_600S_SMTH"
=======
INDIR="${OUTDIR}"
>>>>>>> dacycle_OFP_dev:scale/run/config/realtime_dacycle_D4_250m_large/config.main.ofp

#===============================================================================
# Location of model/data files

SCALEDIR="$DIR/../.."                                                # Directory of the SCALE model
DATADIR="/work/hp150019/share/database" 
SCALE_DB="/work/hp150019/share/database" 

DATA_TOPO=                                                           # Directory of the prepared topo files
DATA_TOPO_BDY_SCALE="${OUTPUT}/${EXP3}/const/topo"
DATA_LANDUSE=                                                        # Directory of the prepared landuse files
DATA_BDY_SCALE="${OUTPUT}/${EXP4}"
DATA_BDY_SCALE_PRC_NUM_X=
DATA_BDY_SCALE_PRC_NUM_Y=
DATA_BDY_SCALE_PREP=                                                 # Directory of the prepared SCALE boundary files
DATA_BDY_WRF=                                                        # Directory of the boundary data in WRF format
DATA_BDY_NICAM=                                                      # Directory of the boundary data in NICAM format (not finished)
DATA_BDY_GRADS=

#OBS="/work/hp150019/f22013/SCALE-LETKF/scale-5.3.2/OUTPUT/obs/PAWR_TOSHIBA"
#OBS="/work/hp150019/f22013/SCALE-LETKF/scale-5.3.2/OUTPUT/obs/PAWR_JRC"
OBS="/work/hp150019/c24140/scale_database/PAWRtest/testcase"
<<<<<<< HEAD:scale/run/config/realtime_dacycle_D4/config.main.ofp
OBS="/work/hp150019/share/honda/SCALE-LETKF/TEST_INPUT/PAWR_TOSHIBA"
OBS= # DEBUG
=======
>>>>>>> dacycle_OFP_dev:scale/run/config/realtime_dacycle_D4_250m_large/config.main.ofp
OBSNCEP=
OBS_USE_JITDT=0 # 1: Run with JIT-DT 
                # 0: No JIT-DT
OBS_USE_JITDT_OFFLINE=0 # 1: Offilne test
                        # 0: Online (data should be transfered from NICT)

JIT_TOP=/work/hp150019/share/honda/jitdt
JIT_TARFILE=${JIT_TOP}/testdata/jit_offline_data_20190610080030-20190610100000.tar
JIT_OFFLOG=${JIT_TOP}/testdata/jit_offline_data_20190610080030-20190610100000.txt

#===============================================================================
# model/data file options

PNETCDF=0               # 0: Do not use PnetCDF
                        # 1: Use PnetCDF

PNETCDF_BDY_SCALE=0     # Use PNETCDF I/O for the parent domain SCALE data? (default: $PNETCDF)
                        # 0: Do not use PnetCDF
                        # 1: Use PnetCDF

DET_RUN=1               # 0: Disable the deterministic run
                        # 1: Enable  the deterministic run

TOPO_FORMAT='prep'      # 'prep': Use prepared topo files in $DATA_TOPO
                        # 'GTOPO30' (requires compatible 'config.nml.scale_pp')
                        # 'DEM50M'  (requires compatible 'config.nml.scale_pp')

LANDUSE_FORMAT='prep'   # 'prep': Use prepared landuse files in $DATA_LANDUSE
                        # 'GLCCv2' (requires compatible 'config.nml.scale_pp')
                        # 'LU100M' (requires compatible 'config.nml.scale_pp')
LANDUSE_UPDATE=0        # 0: Time-invariant landuse files
                        # 1: Time-variant landuse files

BDY_FORMAT=0            # 0: SCALE boundary files (with exactly same domain settings; do not need additional preprocessing)
                        # 1: SCALE history (requires compatible 'config.nml.scale_init')
                        # 2: WRF           (requires compatible 'config.nml.scale_init')
                        # 3: NICAM         (requires compatible 'config.nml.scale_init')
                        # 4: GrADS         (requires compatible 'config.nml.scale_init')
BDY_SINGLE_FILE=1       # 0: Length of a boundary file = $BDYCYCLE_INT (e.g., files made by data assimilation cycles)
                        # 1: Length of a boundary file is long enough so that only a single boundary file is used for each forecast
BDY_SCALE_DIR='fcst'    # Directory name of the SCALE history files when $BDY_FORMAT = 1

BDY_ENS=1               # 0: Fixed boundary files for all memebers
                        # 1: Ensemble boundary files
BDY_ROTATING=0          # 0: Use a same series of boundary files for all initial time
                        # 1: Use different series of boundary files for different initial time

BDYINT=600
BDYCYCLE_INT=21600

PARENT_REF_TIME=

ENABLE_PARAM_USER=1     # 0: Do not enable the 'PARAM_USER' section of the SCALE namelist
                        # 1: Enable the 'PARAM_USER' section of the SCALE namelist (require 'config.nml.scale_user' and customized version of SCALE)

OCEAN_INPUT=0           # 0: No ocean input (use cycling ocean variables)
                        # 1: Update the ocean variables every cycle
OCEAN_FORMAT=0         # 0: SCALE init files (with exactly same domain settings; do not need additional preprocessing)
                        # 99: From the same file as used in generating the boundary conditions ($BDY_FORMAT)
LAND_INPUT=0            # 0: No land input (use cycling land variables)
                        # 1: Update the land variables every cycle
LAND_FORMAT=0          # 0: SCALE init files (with exactly same domain settings; do not need additional preprocessing)
                        # 99: From the same file as used in generating the boundary conditions ($BDY_FORMAT)

OBSNUM=1
OBSNAME[1]=pawr
OBS_FORMAT[1]=PAWR_TOSHIBA
#OBSNAME[1]=ZV
#OBS_FORMAT[1]=PAWR_JRC

#===============================================================================
# Cycling settings

WINDOW_S=30     # SCALE forecast time when the assimilation window starts (second)
WINDOW_E=30     # SCALE forecast time when the assimilation window ends (second)
LCYCLE=30       # Length of a DA cycle (second)
LTIMESLOT=30     # Timeslot interval for 4D-LETKF (second)

#===============================================================================
# Parallelization settings

MEMBER=<MEMBER>      # Ensemble size 
NNODES=<NNODES>


MEMBER=5      # Ensemble size 
NNODES=640   # Ensemble size + 1 (mean) +1 (mdet) + 3 (additional)


PPN=64              # Number of processes per node

THREADS=1          # Number of threads per process

SCALE_NP=4096  # D4    # Number of processes to run SCALE

BGJOB_INT='0.1s'   # Interval of multiple background job submissions

#===============================================================================
# Temporary directories to store runtime files

ONLINE_STGOUT=0             # Stage out right after each cycle (do not wait until the end of the job)?
                            #  0: No
                            #  1: Yes

SYSNAME="$(basename $OUTDIR)"                # A unique name in the machine
TMPSUBDIR="scale-letkf_${SYSNAME}"           # (used to identify multiple runs in the same time)

TMP="$DIR/tmp/$TMPSUBDIR" # Temporary directory shared among all nodes
TMPS="$DIR/tmp/$TMPSUBDIR"  # Temporary directory only on the server node
#TMPS="$TMP"  # Temporary directory only on the server node
TMPL="$TMP"
TMP_JITDATA="/work/hp150019/share/$(id -nu)/jitdt/offline/data"
TMP_JITDATA_IP="$(hostname -i)"

CLEAR_TMP=0                 # Clear temporary directories after the completion of job?
                            #  0: No
                            #  1: Yes

#===============================================================================
# Environmental settings

MPIRUN="mpiexec.hydra"
if (which $MPIRUN > /dev/null 2>&1); then
  MPIRUN=$(which $MPIRUN)
fi

SCP='cp -L'
SCP_HOSTPREFIX=''
#SCP="scp -q"
#SCP_HOSTPREFIX="XXXX:"

STAGE_THREAD=8
TAR_THREAD=8

PYTHON="python"

#BUFRBIN=

#===============================================================================
# Machine-independent source file

. config.rc

#===============================================================================
